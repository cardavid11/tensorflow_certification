{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOlJS5vjCBJ9PZ8WbYTcIFt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cardavid11/tensorflow_certification/blob/main/100%20problems/Problem_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 3 will focus on Image Classification using Convolutional Neural Networks (CNNs) with TensorFlow.\n",
        "\n",
        "### Problem 3: Image Classification with CNNs\n",
        "\n",
        "#### Task Description:\n",
        "Create a Convolutional Neural Network using TensorFlow to perform image classification on the CIFAR-10 dataset.\n",
        "\n",
        "1. **Load Data**: Load the CIFAR-10 dataset.\n",
        "2. **Preprocess Data**: Preprocess the images for CNN training (normalization, resizing if necessary).\n",
        "3. **Build the CNN**:\n",
        "   - Create a CNN model with multiple convolutional and pooling layers.\n",
        "   - Use appropriate activation functions.\n",
        "4. **Compile the Model**:\n",
        "   - Choose an optimizer and loss function suitable for classification.\n",
        "   - Include accuracy as a metric.\n",
        "5. **Data Augmentation (Optional)**: Implement data augmentation to improve model robustness.\n",
        "6. **Train the Model**: Train the model on the training data.\n",
        "7. **Evaluate the Model**: Assess the model's performance on the test set.\n",
        "\n",
        "#### Skills Tested:\n",
        "- Understanding of CNN architecture for image processing.\n",
        "- Familiarity with TensorFlow's Keras API for CNNs.\n",
        "- Preprocessing and data augmentation techniques.\n",
        "- Model training and evaluation.\n",
        "\n",
        "#### Tips:\n",
        "- Pay attention to the input shape required for CNNs.\n",
        "- Experiment with different numbers of layers and filter sizes to improve performance.\n",
        "\n",
        "Let's get started with this CNN-based image classification problem!"
      ],
      "metadata": {
        "id": "wP25DhZztlCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "qiijbVxcvvQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load Data: Load the CIFAR-10 dataset.\n",
        "cifar10 = tf.keras.datasets.cifar10\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
      ],
      "metadata": {
        "id": "vOn_8Nd8vchy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xPruOgcv-Mt",
        "outputId": "9623a142-494d-4458-f911-e880febf3c22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50000, 32, 32, 3), (50000, 1), (10000, 32, 32, 3), (10000, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess Data: Preprocess the images for CNN training (normalization, resizing if necessary).\n",
        "# Normalize the data: rescale pixel values to be between 0 and 1\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n"
      ],
      "metadata": {
        "id": "ocRc2nqixoQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the CNN:\n",
        "# Create a CNN model with multiple convolutional and pooling layers.\n",
        "# Use appropriate activation functions.\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation=\"relu\", input_shape=(32,32,3)),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation=\"relu\"),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(units=120, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(units=10, activation=\"softmax\")\n",
        "])"
      ],
      "metadata": {
        "id": "bXdScAUE94HM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the Model:\n",
        "# Choose an optimizer and loss function suitable for classification.\n",
        "# Include accuracy as a metric.\n",
        "model.compile(optimizer=\"RMSProp\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "Z0sGz6t9Clqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Model: Train the model on the training data.\n",
        "model.fit(x_train, y_train, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5wYkfo5DgRQ",
        "outputId": "44ca6a98-de36-4487-8482-c851448cbf0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 79s 49ms/step - loss: 1.4575 - accuracy: 0.4793\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 1.0685 - accuracy: 0.6276\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 71s 45ms/step - loss: 0.9084 - accuracy: 0.6853\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 72s 46ms/step - loss: 0.7920 - accuracy: 0.7254\n",
            "Epoch 5/10\n",
            "1563/1563 [==============================] - 73s 47ms/step - loss: 0.7003 - accuracy: 0.7590\n",
            "Epoch 6/10\n",
            "1563/1563 [==============================] - 79s 51ms/step - loss: 0.6249 - accuracy: 0.7863\n",
            "Epoch 7/10\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.5595 - accuracy: 0.8065\n",
            "Epoch 8/10\n",
            "1563/1563 [==============================] - 73s 47ms/step - loss: 0.5023 - accuracy: 0.8266\n",
            "Epoch 9/10\n",
            "1563/1563 [==============================] - 71s 45ms/step - loss: 0.4496 - accuracy: 0.8448\n",
            "Epoch 10/10\n",
            "1563/1563 [==============================] - 73s 47ms/step - loss: 0.4027 - accuracy: 0.8616\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x78a8a0c21000>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the Model: Assess the model's performance on the test set.\n",
        "model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "id": "qRSHpzveDmaB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df774cc7-d818-430a-e8a8-48f43770fc70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 4s 14ms/step - loss: 1.0302 - accuracy: 0.6986\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.030211091041565, 0.6985999941825867]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation (Optional): Implement data augmentation to improve model robustness.\n",
        "\n",
        "# Data augmentation is a technique of artificially increasing the training set by creating modified copies of a dataset using existing data. It includes making minor changes to the dataset or using deep learning to generate new data points.  \n",
        "\n",
        "Augmented vs. Synthetic data\n",
        "Augmented data is driven from original data with some minor changes. In the case of image augmentation, we make geometric and color space transformations (flipping, resizing, cropping, brightness, contrast) to increase the size and diversity of the training set.\n",
        "\n",
        "Synthetic data is generated artificially without using the original dataset. It often uses DNNs (Deep Neural Networks) and GANs (Generative Adversarial Networks) to generate synthetic data.\n",
        "\n",
        "Note: the augmentation techniques are not limited to images. You can augment audio, video, text, and other types of data too.\n",
        "\n",
        "When Should You Use Data Augmentation?  \n",
        "To prevent models from overfitting.\n",
        "The initial training set is too small.\n",
        "To improve the model accuracy.\n",
        "To Reduce the operational cost of labeling and cleaning the raw dataset.\n",
        "Limitations of Data Augmentation\n",
        "The biases in the original dataset persist in the augmented data.\n",
        "Quality assurance for data augmentation is expensive.\n",
        "Research and development are required to build a system with advanced applications. For example, generating high-resolution images using GANs can be challenging.\n",
        "Finding an effective data augmentation approach can be challenging.\n",
        "\n",
        "Image Augmentation\n",
        "\n",
        "Geometric transformations: randomly flip, crop, rotate, stretch, and zoom images. You need to be careful about applying multiple transformations on the same images, as this can reduce model performance.\n",
        "Color space transformations: randomly change RGB color channels, contrast, and brightness.\n",
        "Kernel filters: randomly change the sharpness or blurring of the image.\n",
        "Random erasing: delete some part of the initial image.\n",
        "Mixing images: blending and mixing multiple images.\n",
        "\n",
        "Advanced Techniques\n",
        "Generative adversarial networks (GANs): used to generate new data points or images. It does not require existing data to generate synthetic data.\n",
        "Neural Style Transfer: a series of convolutional layers trained to deconstruct images and separate context and style."
      ],
      "metadata": {
        "id": "eRk7hlaKXQz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Class names in CIFAR-10\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# Function to display images and labels\n",
        "def display_images(images, labels, num_images=4):\n",
        "    plt.figure(figsize=(10,10))\n",
        "    for i in range(num_images):\n",
        "        plt.subplot(1, num_images, i+1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.grid(False)\n",
        "        plt.imshow(images[i], cmap=plt.cm.binary)\n",
        "        plt.xlabel(class_names[labels[i][0]])\n",
        "    plt.show()\n",
        "\n",
        "# Display 4 images from x_train and their labels\n",
        "display_images(x_train, y_train)"
      ],
      "metadata": {
        "id": "cyQm4QSGZaxe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "090fd80b-87b8-4849-8ebe-6dbf9e3f0e7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Create an ImageDataGenerator for data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    # rotation_range=15,      # Random rotation between 0 and 15 degrees\n",
        "    # width_shift_range=0.1,  # Random horizontal shifts up to 10% of the width\n",
        "    # height_shift_range=0.1, # Random vertical shifts up to 10% of the height\n",
        "    horizontal_flip=True,   # Random horizontal flips\n",
        "    # You can add more transformations as needed\n",
        ")\n",
        "\n",
        "# Fit the ImageDataGenerator to your training data\n",
        "datagen.fit(x_train)"
      ],
      "metadata": {
        "id": "v__rh9-H1MvJ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation=\"relu\", input_shape=(32,32,3)),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation=\"relu\"),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(units=120, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(units=10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model_with_data_augmentation.compile(optimizer=\"RMSProp\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',   # Monitor validation loss\n",
        "    min_delta=0.001,      # Minimum change to qualify as an improvement\n",
        "    patience=10,          # Stop if no improvement after 10 epochs\n",
        "    mode='min',           # Stop training when the quantity monitored has stopped decreasing\n",
        "    verbose=1,            # Print messages when the callback takes an action\n",
        "    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity.\n",
        ")\n",
        "\n",
        "ds = datagen.flow(x_train, y_train, batch_size=32)\n",
        "\n",
        "# Train the model on the augmented training data\n",
        "model_with_data_augmentation.fit(\n",
        "          ds,\n",
        "          steps_per_epoch=len(x_train) / 32,  # Number of steps per epoch\n",
        "          # If len(x_train) is 50,000 (the size of the CIFAR-10 training dataset) and your batch size is 32, then steps_per_epoch will be 50000 / 32 ≈ 1562.5.\n",
        "          # This means it takes 1563 steps (iterations) to process all training samples in one epoch. Each step processes 32 images.\n",
        "          epochs=20,\n",
        "          validation_data=(x_test, y_test),\n",
        "          callbacks=[early_stopping]\n",
        "          )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "U78vHPDd1Vhd",
        "outputId": "be7edefb-2a89-436c-df3d-38069eb706e5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1562/1562 [==============================] - 82s 52ms/step - loss: 0.6624 - accuracy: 0.1010 - val_loss: 0.8267 - val_accuracy: 0.1048\n",
            "Epoch 9/50\n",
            " 427/1562 [=======>......................] - ETA: 1:01 - loss: 0.6452 - accuracy: 0.1005"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-dc4937437c45>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Train the model on the augmented training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m model_with_data_augmentation.fit(datagen.flow(x_train, y_train, batch_size=32),\n\u001b[0m\u001b[1;32m     24\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Number of steps per epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m           \u001b[0;31m# If len(x_train) is 50,000 (the size of the CIFAR-10 training dataset) and your batch size is 32, then steps_per_epoch will be 50000 / 32 ≈ 1562.5.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1781\u001b[0m                         ):\n\u001b[1;32m   1782\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1783\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1784\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    868\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1262\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1263\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mflat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mflat_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;34m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    253\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1479\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1480\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     ]\n\u001b[0;32m---> 60\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     61\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observing that a model with data augmentation initially performs worse than one without augmentation is not uncommon, especially in the early epochs of training. Here are some reasons why this might happen and what it means for your model:\n",
        "\n",
        "### 1. Increased Complexity\n",
        "- **Data Variation**: Data augmentation introduces additional variability and complexity into the training process. The model now has to learn from a more diverse set of data, which can include transformed versions of the original images.\n",
        "- **Learning Curve**: This increased complexity can make the initial learning process slower. The model is adapting to a broader range of input data, which can initially reflect as lower performance metrics.\n",
        "\n",
        "### 2. Longer Convergence Time\n",
        "- **Adjustment Period**: Due to the variety of augmented data, the model might take more time to start converging towards optimal performance. It's learning more generalized features, which is beneficial in the long run but can appear as lower performance in the short term.\n",
        "- **Epochs Needed**: More epochs might be needed for the model to fully leverage the benefits of data augmentation.\n",
        "\n",
        "### 3. Preventing Overfitting\n",
        "- **Robustness**: While the initial accuracy might be lower, data augmentation typically helps in building a more robust model that generalizes better to new, unseen data.\n",
        "- **Overfitting Reduction**: The model with augmentation might end up with better performance on the validation or test data in the later stages of training, as it's less likely to overfit to the training data.\n",
        "\n",
        "### 4. Tuning Augmentation Parameters\n",
        "- **Balance**: It's crucial to find the right balance in the data augmentation parameters. Overly aggressive augmentation can make the learning task too difficult, while too little might not provide enough variability.\n",
        "- **Experimentation**: You might need to experiment with different types and degrees of augmentation to find what works best for your specific dataset and model.\n",
        "\n",
        "### Conclusion\n",
        "- **Patience and Monitoring**: It's important to be patient and monitor the full course of training. Look at both training and validation performance metrics over time.\n",
        "- **Final Evaluation**: The true test of the model's effectiveness will be its performance on the validation or test set, particularly after it has had enough epochs to learn from the augmented data.\n",
        "- **Adjustment**: If the performance doesn't improve or if the augmentation seems too disruptive, consider adjusting the augmentation parameters.\n",
        "\n",
        "In summary, a temporary drop in performance when using data augmentation can be a normal part of the training process, and the key benefits of augmentation often manifest over a longer period."
      ],
      "metadata": {
        "id": "BDGx8r0KGfX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_data_augmentation.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iJWmeez63Xp",
        "outputId": "76b20d61-a084-407c-9ac6-d2544cd07677"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 6s 19ms/step - loss: 1.4952 - accuracy: 0.0746\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.4951903820037842, 0.07460000365972519]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "is neccesary that I use steps_per_epoch in the fit method?\n",
        "\n",
        "The `steps_per_epoch` parameter in the `fit` method of a TensorFlow model is not always necessary, but its use depends on the specific circumstances of your training setup. Let's explore when and why you might use it:\n",
        "\n",
        "### 1. When Using `ImageDataGenerator` or `tf.data.Dataset`\n",
        "- If you are using an `ImageDataGenerator` or `tf.data.Dataset` for feeding data to your model, it may not always be clear how many samples are in one epoch, especially when applying data augmentation, which can effectively create an 'infinite' dataset.\n",
        "- In such cases, `steps_per_epoch` explicitly specifies how many batches of data (steps) the model should take before considering one epoch completed.\n",
        "\n",
        "### 2. When Not Using Data Generators\n",
        "- If you are not using a data generator and instead passing the entire dataset as a NumPy array directly to the `fit` method, you don't necessarily need to set `steps_per_epoch`.\n",
        "- TensorFlow automatically calculates the number of steps per epoch based on the batch size and the size of your training dataset.\n",
        "\n",
        "### 3. When It's Useful\n",
        "- **Control Over Training**: `steps_per_epoch` gives you control over how much of your training data is used in each epoch. This can be useful for debugging or when you want to see more frequent updates on training progress.\n",
        "- **Large Datasets**: For very large datasets, where you might not want to use the entire dataset each epoch, you can set `steps_per_epoch` to a smaller number to use only a portion of the dataset per epoch, speeding up training iterations.\n",
        "\n",
        "### 4. Default Behavior\n",
        "- By default, if `steps_per_epoch` is not set, it is calculated as `total number of samples / batch size`. This ensures that the model sees the entire dataset in each epoch.\n",
        "\n",
        "### Conclusion\n",
        "- **Use Case Dependent**: Whether you need to set `steps_per_epoch` depends on how you are providing data to your model and whether you need explicit control over the number of steps in each epoch.\n",
        "- **Default Suffices Often**: For many cases, especially with smaller datasets, the default behavior (not setting `steps_per_epoch`) is sufficient.\n",
        "\n",
        "In summary, consider your training setup and specific needs to determine if setting `steps_per_epoch` is necessary for your model training."
      ],
      "metadata": {
        "id": "nrlaYhZ_V2Ji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I feel that the model that implement data augmentation is doing worst ( at least at the beginning, I am still waiting for the rests of epochs). What do you think?\n",
        "\n",
        "Observing that a model with data augmentation initially performs worse than one without augmentation is not uncommon, especially in the early epochs of training. Here are some reasons why this might happen and what it means for your model:\n",
        "\n",
        "### 1. Increased Complexity\n",
        "- **Data Variation**: Data augmentation introduces additional variability and complexity into the training process. The model now has to learn from a more diverse set of data, which can include transformed versions of the original images.\n",
        "- **Learning Curve**: This increased complexity can make the initial learning process slower. The model is adapting to a broader range of input data, which can initially reflect as lower performance metrics.\n",
        "\n",
        "### 2. Longer Convergence Time\n",
        "- **Adjustment Period**: Due to the variety of augmented data, the model might take more time to start converging towards optimal performance. It's learning more generalized features, which is beneficial in the long run but can appear as lower performance in the short term.\n",
        "- **Epochs Needed**: More epochs might be needed for the model to fully leverage the benefits of data augmentation.\n",
        "\n",
        "### 3. Preventing Overfitting\n",
        "- **Robustness**: While the initial accuracy might be lower, data augmentation typically helps in building a more robust model that generalizes better to new, unseen data.\n",
        "- **Overfitting Reduction**: The model with augmentation might end up with better performance on the validation or test data in the later stages of training, as it's less likely to overfit to the training data.\n",
        "\n",
        "### 4. Tuning Augmentation Parameters\n",
        "- **Balance**: It's crucial to find the right balance in the data augmentation parameters. Overly aggressive augmentation can make the learning task too difficult, while too little might not provide enough variability.\n",
        "- **Experimentation**: You might need to experiment with different types and degrees of augmentation to find what works best for your specific dataset and model.\n",
        "\n",
        "### Conclusion\n",
        "- **Patience and Monitoring**: It's important to be patient and monitor the full course of training. Look at both training and validation performance metrics over time.\n",
        "- **Final Evaluation**: The true test of the model's effectiveness will be its performance on the validation or test set, particularly after it has had enough epochs to learn from the augmented data.\n",
        "- **Adjustment**: If the performance doesn't improve or if the augmentation seems too disruptive, consider adjusting the augmentation parameters.\n",
        "\n",
        "In summary, a temporary drop in performance when using data augmentation can be a normal part of the training process, and the key benefits of augmentation often manifest over a longer period."
      ],
      "metadata": {
        "id": "W2STO0J4WC5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selecting the number of epochs for training a neural network model is an important decision that can affect your model's performance. An epoch is one complete pass through the entire training dataset. Here are some guidelines to help you decide on the number of epochs:\n",
        "\n",
        "### Factors to Consider\n",
        "1. **Size of Dataset**:\n",
        "   - Larger datasets often require fewer epochs since more training examples are seen in each epoch.\n",
        "   - Smaller datasets might need more epochs or techniques like data augmentation.\n",
        "\n",
        "2. **Complexity of the Model**:\n",
        "   - More complex models with many parameters might need more epochs to converge.\n",
        "   - Simpler models might converge quickly.\n",
        "\n",
        "3. **Overfitting and Underfitting**:\n",
        "   - Too many epochs can lead to overfitting, especially if you don't have enough training data.\n",
        "   - Too few epochs might leave your model underfitted, not learning enough from the data.\n",
        "\n",
        "4. **Monitoring Tools**:\n",
        "   - Use callbacks like EarlyStopping in TensorFlow to stop training when the validation loss stops decreasing, which can prevent overfitting.\n",
        "\n",
        "### Starting Point\n",
        "- A common approach is to start with a moderate number of epochs, such as 10 to 20, and then adjust based on the model's performance.\n",
        "- Monitor the training and validation loss and accuracy. If the validation accuracy stops improving and the loss stops decreasing, it might be time to stop training.\n",
        "\n",
        "### Example\n",
        "- Start with 15 epochs and monitor the performance.\n",
        "- If the validation accuracy continues to improve, consider increasing the number of epochs.\n",
        "- If overfitting is observed (training accuracy increases but validation accuracy decreases or plateaus), consider using fewer epochs or implementing techniques to combat overfitting.\n",
        "\n",
        "### Using Callbacks for Dynamic Adjustment\n",
        "- Implement callbacks like `EarlyStopping` to automatically stop training when the validation loss stops improving for a specified number of epochs. This is a dynamic way to decide the number of epochs during training.\n",
        "\n",
        "### Conclusion\n",
        "There isn't a one-size-fits-all number of epochs; it depends on your specific model and data. Start with a reasonable estimate, use callbacks for dynamic adjustments, and fine-tune based on the performance metrics you observe."
      ],
      "metadata": {
        "id": "8einyi7yWQFU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "why 32 here \"steps_per_epoch=len(x_train) / 32\"\n",
        "\n",
        "The `32` in the `steps_per_epoch=len(x_train) / 32` statement is the batch size used during model training. Here's a breakdown of what this means:\n",
        "\n",
        "- **Batch Size**: The batch size is the number of training examples used in one iteration of model training. In the code snippet, a batch size of 32 is used, which means the model processes 32 images at a time during training.\n",
        "\n",
        "- **Steps per Epoch**: An epoch is one pass through the entire training dataset. `steps_per_epoch` defines how many batches of samples to use in one epoch.\n",
        "\n",
        "- **Calculation**: By dividing the total number of training samples (`len(x_train)`) by the batch size (32), you get the number of steps (batches of data) needed to cover the entire training set in one epoch.\n",
        "\n",
        "### In Detail:\n",
        "- If `len(x_train)` is 50,000 (the size of the CIFAR-10 training dataset) and your batch size is 32, then `steps_per_epoch` will be `50000 / 32 ≈ 1562.5`.\n",
        "- This means it takes 1563 steps (iterations) to process all training samples in one epoch. Each step processes 32 images.\n",
        "\n",
        "This setup ensures that each sample in your training dataset is seen by the model once per epoch, which is important for effective learning. The choice of batch size can affect your model's performance and training speed, and it's often determined based on memory constraints and the specific requirements of your dataset and model."
      ],
      "metadata": {
        "id": "sVGmJ5b0WVFQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your implementation for the image classification problem using a CNN on the CIFAR-10 dataset looks solid. Here's a brief overview and some suggestions:\n",
        "\n",
        "### Overview of Your Implementation\n",
        "\n",
        "1. **Data Loading and Preprocessing**:\n",
        "   - You've correctly loaded the CIFAR-10 dataset and normalized the pixel values of the images to be between 0 and 1.\n",
        "\n",
        "2. **CNN Architecture**:\n",
        "   - Your model architecture includes convolutional layers (`Conv2D`) with ReLU activation functions and max-pooling layers (`MaxPooling2D`).\n",
        "   - After the convolutional and pooling layers, you've added a `Flatten` layer to convert the 2D features to a 1D vector, followed by dense layers for classification.\n",
        "   - The output layer with 10 units and a softmax activation is appropriate for the 10-class classification task in CIFAR-10.\n",
        "\n",
        "3. **Model Compilation**:\n",
        "   - You are using the RMSProp optimizer and `sparse_categorical_crossentropy` loss function, which is suitable for multi-class classification with integer labels.\n",
        "   - Including accuracy as a metric allows you to monitor the classification performance during training.\n",
        "\n",
        "4. **Model Training**:\n",
        "   - Training for 10 epochs is a reasonable starting point to evaluate how the model performs.\n",
        "\n",
        "5. **Model Evaluation**:\n",
        "   - You're evaluating the model on the test set, which is crucial to understand its performance on unseen data.\n",
        "\n",
        "### Suggestions for Improvement\n",
        "\n",
        "1. **Data Augmentation**:\n",
        "   - Consider implementing data augmentation to improve model generalization. Data augmentation can be particularly helpful for datasets like CIFAR-10, which has diverse image content.\n",
        "\n",
        "2. **Early Stopping and Monitoring**:\n",
        "   - Use callbacks like EarlyStopping to prevent overfitting. This callback will stop the training when the validation accuracy stops improving.\n",
        "   - Monitoring training and validation accuracy and loss during training will help you understand if the model is learning effectively.\n",
        "\n",
        "3. **Experiment with Hyperparameters**:\n",
        "   - Experiment with different hyperparameters like the number of filters, kernel sizes, learning rate of the optimizer, and the number of neurons in the dense layers to find the best combination for your model.\n",
        "   - Adjust the number of epochs based on the performance. If the model has not converged in 10 epochs, you might need to increase this number.\n",
        "\n",
        "4. **Validation Set**:\n",
        "   - If not already done, consider using a separate validation set during training to monitor the model's performance. This helps in tuning the model without touching the test set.\n",
        "\n",
        "5. **Advanced Techniques**:\n",
        "   - Explore more advanced techniques like batch normalization and dropout for further optimization of your model.\n",
        "\n",
        "Overall, you have a good foundational CNN model for the CIFAR-10 dataset. With some fine-tuning and experimentation, you can potentially improve its performance further."
      ],
      "metadata": {
        "id": "RTLjDNQ5XkAP"
      }
    }
  ]
}